{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12745697,"sourceType":"datasetVersion","datasetId":8057173}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3e2a805b-2217-4d7e-a28f-03bb632ac13e","cell_type":"code","source":"import tensorflow as tf\nimport math\nimport os\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:05:30.758695Z","iopub.execute_input":"2025-11-06T08:05:30.758985Z","iopub.status.idle":"2025-11-06T08:05:30.763715Z","shell.execute_reply.started":"2025-11-06T08:05:30.758960Z","shell.execute_reply":"2025-11-06T08:05:30.763015Z"}},"outputs":[],"execution_count":7},{"id":"a7922d14-2321-40ed-9a70-0d5527a2053e","cell_type":"code","source":"print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\nassert tf.config.list_physical_devices('GPU'), \"No GPU detected: Check your Kaggle environment settings.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:05:30.764939Z","iopub.execute_input":"2025-11-06T08:05:30.765205Z","iopub.status.idle":"2025-11-06T08:05:30.782379Z","shell.execute_reply.started":"2025-11-06T08:05:30.765180Z","shell.execute_reply":"2025-11-06T08:05:30.781763Z"}},"outputs":[{"name":"stdout","text":"Num GPUs Available: 1\n","output_type":"stream"}],"execution_count":8},{"id":"3488d20b-ef39-44cc-83e0-ba257bbccb67","cell_type":"code","source":"IMG_SIZE = (224, 224)\nBATCH_SIZE = 32\nDROPOUT_RATE = 0.3\nINITIAL_LR = 0.001\nEPOCHS = 100\nSEED = 42\n\ntrain_dir = '/kaggle/input/cwd30-corn-grass-weeds-new-aug-split-33/data_final_split/train'\nval_dir = '/kaggle/input/cwd30-corn-grass-weeds-new-aug-split-33/data_final_split/val'\ntest_dir = '/kaggle/input/cwd30-corn-grass-weeds-new-aug-split-33/data_final_split/test'\n\nEXPERIMENT_NAME = \"densenet_gam\"  # Options: \"densenet_eca\", \"densenet_ca\", \"densenet_both\"\nATTENTION_TYPE = \"gam\"  # Options: cbam, simam, gam, triplet, shuffle, bam\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:05:30.783104Z","iopub.execute_input":"2025-11-06T08:05:30.783390Z","iopub.status.idle":"2025-11-06T08:05:30.801359Z","shell.execute_reply.started":"2025-11-06T08:05:30.783367Z","shell.execute_reply":"2025-11-06T08:05:30.800622Z"}},"outputs":[],"execution_count":9},{"id":"6b16d6e2-fa83-46a3-a12c-19abf19b7bf8","cell_type":"code","source":"datagen_args = dict(rescale=1./255)\ntrain_gen = ImageDataGenerator(**datagen_args)\nval_gen = ImageDataGenerator(**datagen_args)\ntest_gen = ImageDataGenerator(**datagen_args)\ntrain_flow = train_gen.flow_from_directory(\n    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True, seed=SEED)\nval_flow = val_gen.flow_from_directory(\n    val_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\ntest_flow = test_gen.flow_from_directory(\n    test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\nnum_classes = train_flow.num_classes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:05:30.802582Z","iopub.execute_input":"2025-11-06T08:05:30.802790Z","iopub.status.idle":"2025-11-06T08:05:33.086281Z","shell.execute_reply.started":"2025-11-06T08:05:30.802776Z","shell.execute_reply":"2025-11-06T08:05:33.085497Z"}},"outputs":[{"name":"stdout","text":"Found 14219 images belonging to 43 classes.\nFound 11955 images belonging to 43 classes.\nFound 11958 images belonging to 43 classes.\n","output_type":"stream"}],"execution_count":10},{"id":"9b324fbc-2802-45f7-8b00-3de21c3500cd","cell_type":"code","source":"# ============================================================================\n# GAM (Global Attention Mechanism) - Computational Visual Media 2021\n# Enhanced Channel-Spatial Interaction\n# Paper: \"Global Attention Mechanism: Retain Information to Enhance Channel-Spatial Interactions\"\n# ============================================================================\nclass GAM(layers.Layer):\n    \"\"\"\n    GAM: Global Attention with enhanced channel-spatial interaction\n    Amplifies cross-dimensional interactions\n\n    Args:\n        reduction: channel reduction ratio (default: 16)\n    \"\"\"\n    def __init__(self, reduction=16, **kwargs):\n        super(GAM, self).__init__(**kwargs)\n        self.reduction = reduction\n\n    def build(self, input_shape):\n        channels = input_shape[-1]\n\n        # Channel attention MLP\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(channels // self.reduction, activation='relu'),\n            layers.Dense(channels)\n        ])\n\n        # Spatial attention conv\n        self.conv = layers.Conv2D(channels, kernel_size=7, padding='same')\n\n        super(GAM, self).build(input_shape)\n\n    def call(self, inputs):\n        b, h, w, c = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n\n        # Channel attention\n        x_permute = tf.transpose(inputs, [0, 3, 1, 2])  # [B, C, H, W]\n        x_permute = tf.reshape(x_permute, [b, c, h * w])\n        x_permute = tf.transpose(x_permute, [0, 2, 1])  # [B, H*W, C]\n\n        channel_att = self.mlp(x_permute)\n        channel_att = tf.transpose(channel_att, [0, 2, 1])  # [B, C, H*W]\n        channel_att = tf.reshape(channel_att, [b, c, h, w])\n        channel_att = tf.transpose(channel_att, [0, 2, 3, 1])  # [B, H, W, C]\n        channel_att = tf.nn.sigmoid(channel_att)\n\n        x = inputs * channel_att\n\n        # Spatial attention\n        spatial_att = self.conv(x)\n        spatial_att = tf.nn.sigmoid(spatial_att)\n\n        output = x * spatial_att\n\n        return output\n\n    def get_config(self):\n        config = super(GAM, self).get_config()\n        config.update({\"reduction\": self.reduction})\n        return config\n\n\n\n\ndef build_densenet_with_attention(\n    num_classes,\n    img_size=(224, 224),\n    attention_name='cbam',\n    dropout_rate=0.3\n):\n    \"\"\"\n    Build DenseNet121 with modern attention mechanisms\n\n    Architecture: Input -> DenseNet121 -> ATTENTION -> Dense Layers\n\n    Args:\n        num_classes: number of output classes\n        img_size: input image size\n        attention_name: which attention to use\n        dropout_rate: dropout rate for dense layers\n\n    Returns:\n        Keras Model\n    \"\"\"\n    from tensorflow.keras.applications import DenseNet121\n    from tensorflow.keras import Model\n\n    # Load DenseNet121 backbone\n    base_model = DenseNet121(\n        include_top=False,\n        weights='imagenet',\n        input_shape=img_size + (3,)\n    )\n\n    x = base_model.output\n\n    # Add Attention Module\n    attention_map = {\n        # 'cbam': CBAM,\n        # 'simam': SimAM,\n        'gam': GAM,\n        # 'triplet': TripletAttention,\n        # 'shuffle': ShuffleAttention,\n        # 'bam': BAM\n    }\n\n    if attention_name.lower() in attention_map:\n        AttentionLayer = attention_map[attention_name.lower()]\n        x = AttentionLayer(name=f'{attention_name}_attention')(x)\n        print(f\"âœ“ Added {attention_name.upper()} attention after DenseNet121\")\n    else:\n        print(f\"âš  Unknown attention: {attention_name}. Proceeding without attention.\")\n\n    # Classification Head (matching your original architecture)\n    x = layers.GlobalAveragePooling2D(name='gap')(x)\n    x = layers.Dropout(dropout_rate, name='dropout_1')(x)\n\n    x = layers.Dense(1024, activation='relu', name='fc1')(x)\n    x = layers.BatchNormalization(name='bn1')(x)\n    x = layers.Dropout(dropout_rate, name='dropout_2')(x)\n\n    x = layers.Dense(512, activation='relu', name='fc2')(x)\n    x = layers.BatchNormalization(name='bn2')(x)\n    x = layers.Dropout(dropout_rate, name='dropout_3')(x)\n\n    x = layers.Dense(256, activation='relu', name='fc3')(x)\n    x = layers.BatchNormalization(name='bn3')(x)\n    x = layers.Dropout(dropout_rate, name='dropout_4')(x)\n\n    x = layers.Dense(128, activation='relu', name='fc4')(x)\n\n    output = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n\n    model = Model(\n        inputs=base_model.input,\n        outputs=output,\n        name=f'DenseNet121_{attention_name}'\n    )\n\n    return model\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:05:33.087204Z","iopub.execute_input":"2025-11-06T08:05:33.087492Z","iopub.status.idle":"2025-11-06T08:05:33.102542Z","shell.execute_reply.started":"2025-11-06T08:05:33.087470Z","shell.execute_reply":"2025-11-06T08:05:33.101885Z"}},"outputs":[],"execution_count":11},{"id":"b10d43f8-b6a8-4fc7-bfb1-11eee251cc99","cell_type":"code","source":"print(f\"\\n{'='*70}\")\nprint(f\"Building DenseNet121 with {ATTENTION_TYPE.upper()} attention\")\nprint(f\"{'='*70}\\n\")\n\nmodel = build_densenet_with_attention(\n    num_classes=num_classes,\n    img_size=IMG_SIZE,\n    attention_name=ATTENTION_TYPE,\n    dropout_rate=DROPOUT_RATE\n)\n\n# Make entire model trainable (same as your setup)\nmodel.trainable = True\n\noptimizer = tf.keras.optimizers.AdamW(\n    learning_rate=INITIAL_LR,\n    weight_decay=0.01\n)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:05:33.103387Z","iopub.execute_input":"2025-11-06T08:05:33.103633Z","iopub.status.idle":"2025-11-06T08:05:35.253165Z","shell.execute_reply.started":"2025-11-06T08:05:33.103614Z","shell.execute_reply":"2025-11-06T08:05:35.252465Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nBuilding DenseNet121 with GAM attention\n======================================================================\n\nâœ“ Added GAM attention after DenseNet121\n","output_type":"stream"}],"execution_count":12},{"id":"65c378f5-2322-4929-9283-e23da54cb6e9","cell_type":"code","source":"print(\"\\nModel Summary:\")\nprint(f\"Total parameters: {model.count_params():,}\")\ntrainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Parameter increase vs baseline: +{model.count_params() - 8788843:,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:05:35.253870Z","iopub.execute_input":"2025-11-06T08:05:35.254125Z","iopub.status.idle":"2025-11-06T08:05:35.266829Z","shell.execute_reply.started":"2025-11-06T08:05:35.254099Z","shell.execute_reply":"2025-11-06T08:05:35.266101Z"}},"outputs":[{"name":"stdout","text":"\nModel Summary:\nTotal parameters: 60,302,251\nTrainable parameters: 60,215,019\nParameter increase vs baseline: +51,513,408\n","output_type":"stream"}],"execution_count":13},{"id":"d43e2181-f05e-4040-81ec-faff967c730a","cell_type":"code","source":"checkpoint_cb = ModelCheckpoint(\n    \"best_densenet_model.h5\",\n    save_best_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    verbose=1\n)\n\nearlystop_cb = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=10,\n    restore_best_weights=True,\n    min_delta=0.001,\n    verbose=1\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-7,\n    verbose=1\n)\n\ncallbacks = [checkpoint_cb, earlystop_cb, reduce_lr]\n\nprint(f\"\\n{'='*70}\")\nprint(\"Starting training...\")\nprint(f\"{'='*70}\\n\")\n\nhistory = model.fit(\n    train_flow,\n    epochs=EPOCHS,\n    validation_data=val_flow,\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\nTraining completed!\")\n\n# Save history\nwith open(f'{EXPERIMENT_NAME}_history.json', 'w') as f:\n    json.dump(history.history, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:05:35.267519Z","iopub.execute_input":"2025-11-06T08:05:35.267721Z","execution_failed":"2025-11-06T09:33:38.429Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nStarting training...\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1762416434.397965     127 service.cc:148] XLA service 0x7aabfc04a300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1762416434.398755     127 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1762416443.838615     127 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1762416536.683129     127 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - accuracy: 0.1258 - loss: 3.2902\nEpoch 1: val_accuracy improved from -inf to 0.15475, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 634ms/step - accuracy: 0.1260 - loss: 3.2888 - val_accuracy: 0.1547 - val_loss: 2.8266 - learning_rate: 0.0010\nEpoch 2/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.3716 - loss: 1.7636\nEpoch 2: val_accuracy improved from 0.15475 to 0.38971, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 302ms/step - accuracy: 0.3717 - loss: 1.7632 - val_accuracy: 0.3897 - val_loss: 1.9500 - learning_rate: 0.0010\nEpoch 3/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.5735 - loss: 1.1954\nEpoch 3: val_accuracy improved from 0.38971 to 0.41857, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 303ms/step - accuracy: 0.5735 - loss: 1.1952 - val_accuracy: 0.4186 - val_loss: 2.4758 - learning_rate: 0.0010\nEpoch 4/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.6608 - loss: 0.9086\nEpoch 4: val_accuracy improved from 0.41857 to 0.65780, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 304ms/step - accuracy: 0.6608 - loss: 0.9085 - val_accuracy: 0.6578 - val_loss: 0.9576 - learning_rate: 0.0010\nEpoch 5/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.7276 - loss: 0.7250\nEpoch 5: val_accuracy improved from 0.65780 to 0.71761, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 306ms/step - accuracy: 0.7276 - loss: 0.7250 - val_accuracy: 0.7176 - val_loss: 0.7584 - learning_rate: 0.0010\nEpoch 6/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.7707 - loss: 0.6342\nEpoch 6: val_accuracy did not improve from 0.71761\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 299ms/step - accuracy: 0.7707 - loss: 0.6342 - val_accuracy: 0.6810 - val_loss: 0.9275 - learning_rate: 0.0010\nEpoch 7/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.7955 - loss: 0.5630\nEpoch 7: val_accuracy improved from 0.71761 to 0.76579, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 302ms/step - accuracy: 0.7955 - loss: 0.5630 - val_accuracy: 0.7658 - val_loss: 0.6369 - learning_rate: 0.0010\nEpoch 8/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8272 - loss: 0.4843\nEpoch 9: val_accuracy improved from 0.76579 to 0.81606, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 310ms/step - accuracy: 0.8272 - loss: 0.4843 - val_accuracy: 0.8161 - val_loss: 0.5147 - learning_rate: 0.0010\nEpoch 10/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8519 - loss: 0.4178\nEpoch 10: val_accuracy did not improve from 0.81606\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 297ms/step - accuracy: 0.8519 - loss: 0.4179 - val_accuracy: 0.8100 - val_loss: 0.5094 - learning_rate: 0.0010\nEpoch 11/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8612 - loss: 0.3874\nEpoch 11: val_accuracy improved from 0.81606 to 0.82166, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 301ms/step - accuracy: 0.8612 - loss: 0.3874 - val_accuracy: 0.8217 - val_loss: 0.5354 - learning_rate: 0.0010\nEpoch 12/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8709 - loss: 0.3739\nEpoch 12: val_accuracy improved from 0.82166 to 0.83363, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 301ms/step - accuracy: 0.8709 - loss: 0.3739 - val_accuracy: 0.8336 - val_loss: 0.4961 - learning_rate: 0.0010\nEpoch 13/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8850 - loss: 0.3264\nEpoch 13: val_accuracy improved from 0.83363 to 0.86759, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 301ms/step - accuracy: 0.8850 - loss: 0.3264 - val_accuracy: 0.8676 - val_loss: 0.3770 - learning_rate: 0.0010\nEpoch 14/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8870 - loss: 0.3199\nEpoch 14: val_accuracy did not improve from 0.86759\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 292ms/step - accuracy: 0.8870 - loss: 0.3199 - val_accuracy: 0.8293 - val_loss: 0.4898 - learning_rate: 0.0010\nEpoch 15/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9020 - loss: 0.2756\nEpoch 15: val_accuracy improved from 0.86759 to 0.87562, saving model to best_densenet_model.h5\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 302ms/step - accuracy: 0.9020 - loss: 0.2757 - val_accuracy: 0.8756 - val_loss: 0.3606 - learning_rate: 0.0010\nEpoch 16/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8930 - loss: 0.3052\nEpoch 16: val_accuracy did not improve from 0.87562\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 294ms/step - accuracy: 0.8930 - loss: 0.3052 - val_accuracy: 0.8572 - val_loss: 0.4365 - learning_rate: 0.0010\nEpoch 17/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9143 - loss: 0.2573\nEpoch 17: val_accuracy did not improve from 0.87562\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 299ms/step - accuracy: 0.9143 - loss: 0.2573 - val_accuracy: 0.8018 - val_loss: 0.6120 - learning_rate: 0.0010\nEpoch 18/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9193 - loss: 0.2412\nEpoch 18: val_accuracy did not improve from 0.87562\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 291ms/step - accuracy: 0.9193 - loss: 0.2412 - val_accuracy: 0.7854 - val_loss: 0.6605 - learning_rate: 0.0010\nEpoch 19/100\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9157 - loss: 0.2434\nEpoch 19: val_accuracy did not improve from 0.87562\n\u001b[1m445/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 292ms/step - accuracy: 0.9158 - loss: 0.2434 - val_accuracy: 0.8246 - val_loss: 0.5856 - learning_rate: 0.0010\nEpoch 20/100\n\u001b[1m241/445\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m41s\u001b[0m 204ms/step - accuracy: 0.9170 - loss: 0.2359","output_type":"stream"}],"execution_count":null},{"id":"b979cb98-419d-416c-af9a-aa1c9f45918b","cell_type":"code","source":"print(\"\\nEvaluating on test set...\")\n\ntest_flow.reset()\ny_true = test_flow.classes\ny_pred_probs = model.predict(test_flow, verbose=1)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\ntarget_names = list(test_flow.class_indices.keys())\nreport = classification_report(y_true, y_pred, target_names=target_names, digits=4, output_dict=True)\ncm = confusion_matrix(y_true, y_pred)\ntest_loss, test_acc = model.evaluate(test_flow, verbose=0)\n\n# Metrics\nprecision = report[\"macro avg\"][\"precision\"]\nrecall = report[\"macro avg\"][\"recall\"]\nf1 = report[\"macro avg\"][\"f1-score\"]\nnum_params = model.count_params()\n\n\nprint(f\"\\n{'='*70}\")\nprint(\"FINAL RESULTS\")\nprint(f\"{'='*70}\")\nprint(f\"Experiment: {EXPERIMENT_NAME}\")\nprint(f\"Attention Type: {ATTENTION_TYPE.upper()}\")\nprint(f\"{'-'*70}\")\nprint(f\"Test Loss:      {test_loss:.4f}\")\nprint(f\"Test Accuracy:  {test_acc:.4f}\")\nprint(f\"Precision:      {precision:.4f}\")\nprint(f\"Recall:         {recall:.4f}\")\nprint(f\"F1 Score:       {f1:.4f}\")\nprint(f\"Parameters:     {num_params:,}\")\nprint(f\"Param increase: +{num_params - 8788843:,}\")\nprint(f\"{'='*70}\\n\")\n\n# Compare with baseline\nbaseline_acc = 0.9489\nbaseline_f1 = 0.9235\nprint(\"COMPARISON WITH BASELINE:\")\nprint(f\"Accuracy improvement: {(test_acc - baseline_acc)*100:+.2f}%\")\nprint(f\"F1 improvement:       {(f1 - baseline_f1)*100:+.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T10:06:20.856582Z","iopub.execute_input":"2025-11-06T10:06:20.857286Z","iopub.status.idle":"2025-11-06T10:06:20.868407Z","shell.execute_reply.started":"2025-11-06T10:06:20.857258Z","shell.execute_reply":"2025-11-06T10:06:20.867429Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating on test set...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3619758818.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating on test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_flow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_flow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_flow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_flow' is not defined"],"ename":"NameError","evalue":"name 'test_flow' is not defined","output_type":"error"}],"execution_count":1},{"id":"bf76f7f5-cf00-41ac-a1a5-5f446c706fa8","cell_type":"code","source":"results = {\n    \"Model\": [EXPERIMENT_NAME],\n    \"Attention\": [ATTENTION_TYPE],\n    \"Params\": [num_params],\n    \"Param_increase\": [num_params - 8788843],\n    \"Test_Loss\": [f\"{test_loss:.4f}\"],\n    \"Test_Acc\": [f\"{test_acc:.4f}\"],\n    \"Precision\": [f\"{precision:.4f}\"],\n    \"Recall\": [f\"{recall:.4f}\"],\n    \"F1\": [f\"{f1:.4f}\"],\n    \"Acc_vs_baseline\": [f\"{(test_acc - baseline_acc)*100:+.2f}%\"],\n    \"F1_vs_baseline\": [f\"{(f1 - baseline_f1)*100:+.2f}%\"]\n}\n\nimport pandas as pd\ndf = pd.DataFrame(results)\nprint(\"\\n\" + df.to_markdown(index=False))\n\n# Save to CSV for easy comparison\ndf.to_csv(f'{EXPERIMENT_NAME}_results.csv', index=False)\n\n# Save model\nmodel.save(f'{EXPERIMENT_NAME}_final_model.keras')\nmodel.save_weights(f'{EXPERIMENT_NAME}_final.weights.h5')\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-06T09:33:38.430Z"}},"outputs":[],"execution_count":null},{"id":"46b17ffe-4b8f-46ff-8169-23075f4e90cf","cell_type":"code","source":"# Confusion Matrix\nplt.figure(figsize=(12, 10))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title(f\"Confusion Matrix - {EXPERIMENT_NAME}\")\nplt.colorbar()\ntick_marks = np.arange(len(target_names))\nplt.xticks(tick_marks, target_names, rotation=90)\nplt.yticks(tick_marks, target_names)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.savefig(f'{EXPERIMENT_NAME}_confusion_matrix.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Loss curves\nplt.figure(figsize=(10, 5))\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.axhline(y=test_loss, color='r', linestyle='--', label=f'Test Loss: {test_loss:.4f}')\nplt.legend()\nplt.title(f\"Loss over epochs - {EXPERIMENT_NAME}\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid(True, alpha=0.3)\nplt.savefig(f'{EXPERIMENT_NAME}_loss.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Accuracy curves\nplt.figure(figsize=(10, 5))\nplt.plot(history.history[\"accuracy\"], label=\"Training Acc\")\nplt.plot(history.history[\"val_accuracy\"], label=\"Validation Acc\")\nplt.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Acc: {test_acc:.4f}')\nplt.axhline(y=baseline_acc, color='g', linestyle=':', label=f'Baseline: {baseline_acc:.4f}')\nplt.legend()\nplt.title(f\"Accuracy over epochs - {EXPERIMENT_NAME}\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True, alpha=0.3)\nplt.savefig(f'{EXPERIMENT_NAME}_accuracy.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nAll results saved with prefix: {EXPERIMENT_NAME}_\")\nprint(\"Training complete! ğŸ‰\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-06T09:33:38.431Z"}},"outputs":[],"execution_count":null},{"id":"289a1621-731b-4429-9c00-40b1fa3b66e4","cell_type":"code","source":"curl -d \"Hi\" ntfy.sh/model_complete","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-06T09:33:38.431Z"}},"outputs":[],"execution_count":null}]}